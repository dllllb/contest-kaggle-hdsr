{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "from nltk import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import matutils\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(est, features, labels):\n",
    "    pred = est.predict(features)\n",
    "    return mean_squared_error(labels, pred)**.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_similarity(wv, ws1, ws2):\n",
    "    v1 = [v for v in [wv[word] for word in ws1 if word in wv] if v is not None]\n",
    "    v2 = [v for v in [wv[word] for word in ws2 if word in wv] if v is not None]\n",
    "\n",
    "    if v1 and v2:\n",
    "        return np.dot(matutils.unitvec(np.array(v1).mean(axis=0)), matutils.unitvec(np.array(v2).mean(axis=0)))\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(s):\n",
    "    s = re.sub(r\"(\\w)\\.([A-Z])\", r\"\\1 \\2\", s)  # Split words with a.A\n",
    "    s = s.lower()\n",
    "\n",
    "    s = re.sub(r\"([0-9]+)( *)(inches|inch|in|')\\.?\", r\"\\1in. \", s)\n",
    "    s = re.sub(r\"([0-9]+)( *)(foot|feet|ft|'')\\.?\", r\"\\1ft. \", s)\n",
    "    s = re.sub(r\"([0-9]+)( *)(pounds|pound|lbs|lb)\\.?\", r\"\\1lb. \", s)\n",
    "\n",
    "    s = s.replace(\" x \", \" xby \")\n",
    "    s = s.replace(\"*\", \" xby \")\n",
    "    s = s.replace(\" by \",\" xby\")\n",
    "    s = s.replace(\"x0\", \" xby 0\")\n",
    "    s = s.replace(\"x1\", \" xby 1\")\n",
    "    s = s.replace(\"x2\", \" xby 2\")\n",
    "    s = s.replace(\"x3\", \" xby 3\")\n",
    "    s = s.replace(\"x4\", \" xby 4\")\n",
    "    s = s.replace(\"x5\", \" xby 5\")\n",
    "    s = s.replace(\"x6\", \" xby 6\")\n",
    "    s = s.replace(\"x7\", \" xby 7\")\n",
    "    s = s.replace(\"x8\", \" xby 8\")\n",
    "    s = s.replace(\"x9\", \" xby 9\")\n",
    "    s = s.replace(\"0x\", \"0 xby \")\n",
    "    s = s.replace(\"1x\", \"1 xby \")\n",
    "    s = s.replace(\"2x\", \"2 xby \")\n",
    "    s = s.replace(\"3x\", \"3 xby \")\n",
    "    s = s.replace(\"4x\", \"4 xby \")\n",
    "    s = s.replace(\"5x\", \"5 xby \")\n",
    "    s = s.replace(\"6x\", \"6 xby \")\n",
    "    s = s.replace(\"7x\", \"7 xby \")\n",
    "    s = s.replace(\"8x\", \"8 xby \")\n",
    "    s = s.replace(\"9x\", \"9 xby \")\n",
    "\n",
    "    s = re.sub(r\"([0-9]+)( *)(square|sq) ?\\.?(feet|foot|ft)\\.?\", r\"\\1sq.ft. \", s)\n",
    "    s = re.sub(r\"([0-9]+)( *)(gallons|gallon|gal)\\.?\", r\"\\1gal. \", s)\n",
    "    s = re.sub(r\"([0-9]+)( *)(ounces|ounce|oz)\\.?\", r\"\\1oz. \", s)\n",
    "    s = re.sub(r\"([0-9]+)( *)(centimeters|cm)\\.?\", r\"\\1cm. \", s)\n",
    "    s = re.sub(r\"([0-9]+)( *)(milimeters|mm)\\.?\", r\"\\1mm. \", s)\n",
    "    s = re.sub(r\"([0-9]+)( *)(degrees|degree)\\.?\", r\"\\1deg. \", s)\n",
    "    s = re.sub(r\"([0-9]+)( *)(volts|volt)\\.?\", r\"\\1volt. \", s)\n",
    "    s = re.sub(r\"([0-9]+)( *)(watts|watt)\\.?\", r\"\\1watt. \", s)\n",
    "    s = re.sub(r\"([0-9]+)( *)(amperes|ampere|amps|amp)\\.?\", r\"\\1amp. \", s)\n",
    "\n",
    "    s = s.replace(\"whirpool\", \"whirlpool\")\n",
    "    s = s.replace(\"whirlpoolga\", \"whirlpool\")\n",
    "    s = s.replace(\"whirlpoolstainless\",\"whirlpool stainless\")\n",
    "\n",
    "    s = s.replace(\"  \", \" \")\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleanupStemTokenizer:\n",
    "    def __init__(self):\n",
    "        self.stemmer = SnowballStemmer('english')\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        clean_text = cleanup(text).decode('utf8')\n",
    "        words = [self.stemmer.stem(w) for w in clean_text.lower().split()]\n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopwordTokenizer:\n",
    "    def __init__(self):\n",
    "        self.stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        clean_text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "        words = clean_text.lower().split()\n",
    "        words = [w for w in words if w not in self.stopwords]\n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class W2VTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, tokenizer):\n",
    "        self.wv = None\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def transform(self, df):\n",
    "\n",
    "        def similarity(left, right):\n",
    "            return [n_similarity(self.wv, set(l), set(r)) for l, r in zip(left, right)]\n",
    "\n",
    "        query_terms = df.search_term.map(self.tokenizer.tokenize)\n",
    "        title_terms = df.product_title.map(self.tokenizer.tokenize)\n",
    "        desc_terms = df.product_description.map(self.tokenizer.tokenize)\n",
    "\n",
    "        title_sim = similarity(query_terms, title_terms)\n",
    "        desc_sim = similarity(query_terms, desc_terms)\n",
    "\n",
    "        res = pd.DataFrame({'title_sim': title_sim, 'desc_sim': desc_sim})\n",
    "        return res\n",
    "\n",
    "    def fit(self, df, y=None, **fit_params):\n",
    "        sentences = df.product_title.map(self.tokenizer.tokenize)\n",
    "        sentences.append(df.product_description.map(self.tokenizer.tokenize))\n",
    "\n",
    "        self.wv = Word2Vec(sentences, min_count=3, workers=4)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_terms_count(left, right):\n",
    "    return [len(set(l).intersection(set(r))) for l, r in zip(left, right)]\n",
    "\n",
    "class QueryMatchTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.tokenizer = CleanupStemTokenizer()\n",
    "\n",
    "    def transform(self, df):\n",
    "        res = pd.DataFrame()\n",
    "\n",
    "        query_terms = df.search_term.map(self.tokenizer.tokenize)\n",
    "        title_terms = df.product_title.map(self.tokenizer.tokenize)\n",
    "        desc_terms = df.product_description.map(self.tokenizer.tokenize)\n",
    "        brand_terms = df.brand.map(self.tokenizer.tokenize)\n",
    "\n",
    "        res['query_len'] = query_terms.map(len)\n",
    "        res['title_len'] = title_terms.map(len)\n",
    "        res['desc_len'] = desc_terms.map(len)\n",
    "        res['brand_len'] = brand_terms.map(len)\n",
    "\n",
    "        res['query_words_in_title'] = same_terms_count(query_terms, title_terms)\n",
    "        res['query_words_in_desc'] = same_terms_count(query_terms, desc_terms)\n",
    "        res['query_words_in_brand'] = same_terms_count(query_terms, brand_terms)\n",
    "\n",
    "        cleaned_query = df.search_term.map(cleanup)\n",
    "        cleaned_title = df.product_title.map(cleanup)\n",
    "        cleaned_desc = df.product_description.map(cleanup)\n",
    "\n",
    "        res['query_in_title'] = [l.count(r) for l, r in zip(cleaned_title, cleaned_query)]\n",
    "        res['query_in_desc'] = [l.count(r) for l, r in zip(cleaned_desc, cleaned_query)]\n",
    "\n",
    "        res['ratio_title'] = res['query_words_in_title']/res['query_len']\n",
    "        res['ratio_description'] = res['query_words_in_desc']/res['query_len']\n",
    "        res['ratio_brand'] = res['query_words_in_brand']/res['query_len']\n",
    "\n",
    "        return res\n",
    "\n",
    "    def fit(self, df, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryMatchAttrTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.tokenizer = CleanupStemTokenizer()\n",
    "\n",
    "    def transform(self, df):\n",
    "        res = pd.DataFrame()\n",
    "\n",
    "        query_terms = df.search_term.map(self.tokenizer.tokenize)\n",
    "        attr_terms = df.attrs.map(self.tokenizer.tokenize)\n",
    "\n",
    "        res['query_len'] = query_terms.map(len)\n",
    "        res['attr_len'] = attr_terms.map(len)\n",
    "\n",
    "        res['query_words_in_attr'] = same_terms_count(query_terms, attr_terms)\n",
    "\n",
    "        cleaned_query = df.search_term.map(cleanup)\n",
    "        cleaned_attrs = df.product_description.map(cleanup)\n",
    "\n",
    "        res['query_in_attr'] = [l.count(r) for l, r in zip(cleaned_attrs, cleaned_query)]\n",
    "\n",
    "        res['ratio_attr'] = res['query_words_in_attr']/res['query_len']\n",
    "\n",
    "        return res\n",
    "\n",
    "    def fit(self, df, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizer_sum(vectorizer, query, field):\n",
    "    q_vecs = vectorizer.transform(query)\n",
    "    f_vecs = vectorizer.transform(field) > 0\n",
    "\n",
    "    return q_vecs.multiply(f_vecs).sum(axis=1).A1\n",
    "\n",
    "\n",
    "class QueryMatchScoreTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.tokenizer = CleanupStemTokenizer()\n",
    "        self.title_vec = None\n",
    "        self.desc_vec = None\n",
    "        self.brand_vec = None\n",
    "\n",
    "    def transform(self, df):\n",
    "        res = pd.DataFrame()\n",
    "\n",
    "        query_len = df.search_term.map(self.tokenizer.tokenize).map(len)\n",
    "\n",
    "        res['query_title_tfidf'] = vectorizer_sum(self.title_vec, df.search_term, df.product_title)\n",
    "        res['query_desc_tfidf'] = vectorizer_sum(self.desc_vec, df.search_term, df.product_description)\n",
    "        res['query_brand_tfidf'] = vectorizer_sum(self.brand_vec, df.search_term, df.brand)\n",
    "\n",
    "        res['tfidf_ratio_title'] = res['query_title_tfidf']/query_len\n",
    "        res['tfidf_ratio_description'] = res['query_desc_tfidf']/query_len\n",
    "        res['tfidf_ratio_brand'] = res['query_brand_tfidf']/query_len\n",
    "\n",
    "        return res\n",
    "\n",
    "    def fit(self, df, y=None, **fit_params):\n",
    "        self.title_vec = TfidfVectorizer(analyzer=self.tokenizer.tokenize).fit(df.product_title)\n",
    "        self.desc_vec = TfidfVectorizer(analyzer=self.tokenizer.tokenize).fit(df.product_description)\n",
    "        self.brand_vec = TfidfVectorizer(analyzer=self.tokenizer.tokenize).fit(df.brand)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
